
<!DOCTYPE HTML>
<html>
	<head>
		<title>Andy Zeng - Princeton University</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
          ga('create', 'UA-89797207-1', 'auto');
          ga('send', 'pageview');
        </script>
	</head>
	<body id="top">

		<!-- Header -->
			<header id="header">
				<div class="inner">
					<a href="#" class="image avatar"><img src="images/avatar.jpg" alt="" /></a>
					<h1><strong>Andy Zeng</strong><br></h1>
					<!-- <h1>PhD Student @ <a href="http://www.cs.princeton.edu/">Princeton</a><br></h1>
					<h1><sup>advised by <a href="https://www.cs.princeton.edu/~funk/">Tom Funkhouser</a></sup><br></h1>
					<h1>Graduated from <a href="https://www.berkeley.edu/">Berkeley</a><br></h1>
					<h1><sup>Bachelors in CS and Math</sup><br></h1>
					<h1>Works on AI &amp; Robotics<br></h1>
					<h1><sup>@ <a href="http://3dvision.princeton.edu/">Princeton Vision Group</a></sup><br></h1>
					<h1 style="margin-top: -0.5em; margin-bottom: 0.5em"><sup>@ <a href="https://research.google.com/">Google Brain Robotics</a></sup> -->
					<h1 style="margin-top: 0.4em"><sup class="icon fa-envelope">&nbsp;&nbsp;andyz_at_princeton.edu</sup><br></h1>
					<h1><sup><a href="https://www.linkedin.com/in/andy-zeng-6575b85b" class="icon fa-linkedin">&nbsp;&nbsp;LinkedIn</a></sup>&nbsp;&nbsp;&nbsp;&nbsp;<sup><a href="https://twitter.com/andyzengtweets" class="icon fa-twitter">&nbsp;&nbsp;Twitter</a></sup></h1>
					<h1><sup><a href="https://scholar.google.com/citations?user=q7nFtUcAAAAJ&hl=en" class="icon fa-graduation-cap">&nbsp;&nbsp;G. Scholar</a></sup>&nbsp;&nbsp;&nbsp;&nbsp;<sup><a href="https://github.com/andyzeng" class="icon fa-github">&nbsp;&nbsp;Github</a></sup></h1>
					<h1><sup><a href="resume.pdf" class="icon fa-suitcase">&nbsp;&nbsp;Resume</a></sup></h1>
					<!-- <ul class="icons">
						<li><a href="resume.pdf" class="icon" style="font-weight: 700"><font size="6">cv</font></a></li>
						<li><a href="https://twitter.com/andyzengtweets" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
						<li><a href="https://scholar.google.com/citations?user=q7nFtUcAAAAJ&hl=en" class="icon fa-graduation-cap"><span class="label">G. Scholar</span></a></li>
						<li><a href="https://www.linkedin.com/in/andy-zeng-6575b85b" class="icon fa-linkedin"><span class="label">Github</span></a></li>
						<li><a href="https://github.com/andyzeng" class="icon fa-github"><span class="label">Github</span></a></li>
					</ul> -->
				</div>
			</header>

		<!-- Main -->
			<div id="main" style="padding-bottom:1em">
					<section id="one">
						<!-- <header class="major">
							<h2>Giving robots the gift of sight.</h2>
						</header> -->
							<!-- <h2>I'm passionate about building robots that can</h2>
							<ul>
                                <li>Understand the visual world around us</li>
                                <li>Intelligently interact with the physical world</li>
                                <li>Teach themselves new skills and improve over time</li>
                            </ul> -->
							<h2>About</h2>
							<p>I'm a third-year PhD student at <a href="http://3dvision.princeton.edu/">Princeton</a> where I work with <a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>. I'm currently spending time at <a href="https://research.google.com/">Google Brain Robotics</a>. Previously, I graduated from <a href="https://www.berkeley.edu/">UC Berkeley</a> with a Bachelors double major in Computer Science and Mathematics. I work on AI and robotics.
							</p>
							<hr/>
							<h2>News</h2>
							<ul>
                                <li>Thrilled to be a recipient of the <a href="https://blogs.nvidia.com/blog/2018/04/04/nvidia-graduate-fellowship-program/">NVIDIA Fellowship</a>. Thanks NVIDIA!</li>
                                <!-- <li><a href="http://im2pano3d.cs.princeton.edu/">Im2Pano3D</a> will appear as a publication (oral) at CVPR 2018</li> -->
                                <!-- <li>Will be a visiting researcher at <a href="https://research.google.com/">Google Brain Robotics</a>. In the bay area? <a href="mailto:andyzeng@google.com">Let's chat</a>!</li> -->
								<li>1<sup>st</sup> place winners of the stow task at the worldwide <a href="https://www.amazonrobotics.com/">Amazon Robotics Challenge 2017</a> with <a href="https://mcube.mit.edu/">MIT</a>!
								<!-- <ul style="margin-bottom: 0">
									<i><li><a href="http://arc.cs.princeton.edu/">Our work</a> on grasping and one-shot recognition of novel objects will appear as a publication at ICRA 2018</li>
									<li>Exciting media coverage on our work: <a href="http://news.mit.edu/2018/robo-picker-grasps-and-packs-0220">MIT-Princeton's Robo-picker Grasps and Packs</a></li></i>
								</ul> -->
								</li>
								<!-- <li>Both <a href="http://3dmatch.cs.princeton.edu/">3DMatch</a> and <a href="http://sscnet.cs.princeton.edu/">Semantic Scene Completion</a> will appear as publications (orals) at CVPR 2017</li> -->
							</ul>
						<hr/>
						<h2>Recent Work</h2>
						<div style="width: 100%; height: 14em">
							<div style="border-radius: 0.35em; float: left; width: 31%; height: 9.5em; margin-right: 3%; background: rgba(0,0,0,0.5);">
								<video width="100%" playsinline muted autoplay loop style="border-radius: 0.35em; height: 9.5em; opacity: 0.8;">
								  <source src="videos/closed-loop-grasping.mp4" type="video/mp4">
								</video>
							</div>
							<div style="border-radius: 0.35em; float: left; width: 31%; height: 9.5em; margin-right: 3%; background: rgba(0,0,0,0.5);">
								<video width="100%" playsinline muted autoplay loop style="border-radius: 0.35em; height: 9.5em; opacity: 0.8;">
								  <source src="videos/visual-pushing-grasping.mp4" type="video/mp4">
								</video>
							</div>
							<div style="border-radius: 0.35em; float: left; width: 31%; height: 9.5em; background: rgba(0,0,0,0.5);">
								<video width="100%" playsinline muted autoplay loop style="border-radius: 0.35em; height: 9.5em; opacity: 0.8;">
								  <source src="videos/amazon-robotics-challenge.mp4" type="video/mp4">
								</video>
							</div>
							<div style="float: left; width: 31%; margin-right: 3%; font-size: 0.8em; line-height: 1.5em; margin-top: 1em;">Reactive real-time perception and control.<br>"How can we teach robots to react and adapt to our dynamic world?"</div>
							<div style="float: left; width: 31%; margin-right: 3%; font-size: 0.8em; line-height: 1.5em; margin-top: 1em;">Intelligent planning and manipulation.<br>"How can robots self-learn complex dexterous skills?" <a href="http://vpg.cs.princeton.edu/">project link</a></div>
							<div style="float: left; width: 31%; font-size: 0.8em; line-height: 1.5em; margin-top: 1em;">Industrial applications e.g. pick-and-place.<br>"How do we bring deep robotic learning technologies to industry?" <a href="http://arc.cs.princeton.edu/">project link</a></div>
						</div>
						<hr/>
						<h2>Talks</h2>
						<!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/qNVZl7bCjsU?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe><br>

						Oral presentation on <a href="http://3dmatch.cs.princeton.edu/">3DMatch</a> at CVPR 2017 (slides <a href="slides/3dmatch-cvpr2017.pdf">here</a>)<br><br> -->
						<ul>
		                    <li><strong>June 2018.</strong> Invited talk at the <a href="https://www.re-work.co/events/deep-learning-for-robotics-summit-san-francisco-2018/schedule#day_2">Deep Learning for Robotics Summit 2018</a></li>
		                    <li><strong>Nov. 2017.</strong> Guest lecture on self-supervised deep learning at the Robotics Seminar at NCTU: Robotic Manipulation - Perception, Planning and Design</li>
		                    <li><strong>Nov. 2017.</strong> Invited talks at <a href="https://x.company/">Google X</a> and <a href="https://research.google.com/teams/brain/">Google Brain</a> (slides <a href="slides/picking-novel-objects-google-brain.pdf">here</a> and <a href="slides/model-free-grasping-google-x.pdf">here</a>)</li>
		                    <li><strong>July 2017.</strong> Oral talk on <a href="http://3dmatch.cs.princeton.edu/">3DMatch</a> at CVPR 2017 (video <a href="https://youtu.be/qNVZl7bCjsU">here</a> and slides <a href="slides/3dmatch-cvpr2017.pdf">here</a>)</li>
		                    <li><strong>Mar. and Apr. 2017.</strong> Talks at the <a href="http://www.cs.princeton.edu/courses/archive/spring17/cos598F/index.html">Deep Learning for Graphics and Vision Seminar</a> at Princeton</li>
		                    <li><strong>July 2016.</strong> Invited talk on lessons learned from the Amazon Picking Challenge 2016</li>
		                    <li><strong>June 2016.</strong> Contributed talk at the first <a href="http://3dvision.princeton.edu/event/cvpr16/3DDeepLearning/">3D Deep learning Tutorial</a> at CVPR 2016</li>
	                	</ul>
						<hr/>
						<h2>Research</h2>
						<!-- <p> My research is to develop algorithms that enable robots to intelligently interact with the physical world and improve themselves over time. In particular, I am interested in building reactive perception-driven systems that can autonomously acquire the sensorimotor skills necessary to execute complex tasks in unstructured environments. My work lies at the intersection of robotics, computer vision, and machine learning - with emphasis on self-supervised deep learning.</p> -->
						<p>I work on deep learning for robotic perception and manipulation. In particular, I'm interested in developing algorithms that enable machines to intelligently interact with the physical world and improve themselves over time. My work lies at the intersection of robotics, computer vision, and machine learning.</p>
						<div class="row">
							<article class="12u 12u$(xsmall) work-item">
								<a href="http://vpg.cs.princeton.edu/" class="image fit thumb left"><img src="http://vision.princeton.edu/projects/2018/vpg/images/teaser.jpg" alt="" /></a>
								<h3 style="margin-left: 43%">Learning Synergies between Pushing and Grasping with Self-supervised Deep Reinforcement Learning</h3>
								<p style="margin-left: 43%">
								Skilled robotic manipulation benefits from complex synergies between non-prehensile (e.g. pushing) and prehensile (e.g. grasping) actions: pushing can help rearrange cluttered objects to make space for arms and fingers; likewise, grasping can help displace objects to make pushing movements more precise and collision-free. In this work, we demonstrate that it is possible to discover and learn these synergies from scratch by combining visual affordance-based manipulation with model-free deep reinforcement learning. Our method is sample efficient and generalizes to novel objects and scenarios.
								<br>
								</p>
								<p style="margin-left: 43%; margin-top: 1em">
								<font color="49bf9d">Andy Zeng</font>, Shuran Song, Stefan Welker, Johnny Lee, Alberto Rodriguez, Thomas Funkhouser<br>
								<i>IEEE International Conference on Intelligent Robots and Systems (IROS) 2018</i><br>
                                    <a href="http://vpg.cs.princeton.edu/">Webpage</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
									<a href="https://arxiv.org/pdf/1803.09956.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                                    <a href="https://github.com/andyzeng/visual-pushing-grasping">Code (Github)</a>
								</p>
							</article>
							<article class="12u 12u$(xsmall) work-item">
								<a href="http://arc.cs.princeton.edu" class="image fit thumb left"><img src="http://vision.princeton.edu/projects/2017/arc/teaser.jpg" alt="" /></a>
								<h3 style="margin-left: 43%">Robotic Pick-and-Place of Novel Objects in Clutter with Multi-Affordance Grasping and Cross-Domain Image Matching</h3>
								<p style="margin-left: 43%">
								We built a robo-picker that can grasp and recognize novel objects (appearing for the first time during testing) in cluttered environments without needing any additional data collection or re-training. It achieves this with pixel-wise affordance-based grasping and one-shot learning to recognize objects using only product images (e.g., from the web). The approach was part of the MIT-Princeton Team system that took <font color="4e79a7">1<sup>st</sup> place in the stowing task at the 2017 Amazon Robotics Challenge.</font> 
								<br>
								</p>
								<p style="margin-left: 43%; margin-top: 1em">
								<font color="49bf9d">Andy Zeng</font>, Shuran Song, Kuan-Ting Yu, Elliott Donlon, Francois R. Hogan, Maria Bauza, Daolin Ma, Orion Taylor, Melody Liu, Eudald Romo, Nima Fazeli, Ferran Alet, Nikhil Chavan Dafle, Rachel Holladay, Isabella Morona, Prem Qu Nair, Druck Green, Ian Taylor, Weber Liu, Thomas Funkhouser, Alberto Rodriguez<br>
								<i>IEEE International Conference on Robotics and Automation (ICRA) 2018</i><br>
                                    <a href="http://arc.cs.princeton.edu">Project</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
									<a href="https://arxiv.org/pdf/1710.01330.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                                    <a href="https://github.com/andyzeng/arc-robot-vision">Code (Github)</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                                    <a href="http://news.mit.edu/2018/robo-picker-grasps-and-packs-0220">MIT News</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                                    <a href="https://www.youtube.com/watch?v=yVIRLao1E28">Amazon News</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                                    <a href="https://www.engadget.com/2018/02/20/robot-pick-up-sort-object-improve-warehouse-efficiency/">Engadget</a>
								</p>
							</article>
							<article class="12u 12u$(xsmall) work-item">
								<a href="http://im2pano3d.cs.princeton.edu" class="image fit thumb left"><img src="img/proj/rs/im2pano3d/task.jpg" alt="" /></a>
								<h3 style="margin-left: 43%">Im2Pano3D: Extrapolating 360&deg; Structure and Semantics Beyond the Field of View</h3>
								<p style="margin-left: 43%">
								We explore the limits of leveraging strong contextual priors learned from large-scale synthetic and real-world indoor scenes. To this end, we trained a network that can generate a dense prediction of 3D structure and a probability distribution of semantic labels for a full 360&deg; panoramic view of an indoor scene when given only a partial observation (<= 50%) in the form of an RGB-D image -- i.e., it can infer what's behind you.
								</p>
								<p style="margin-left: 43%; margin-top: 1em">
								Shuran Song, <font color="49bf9d">Andy Zeng</font>, Angel X. Chang, Manolis Savva, Silvio Savarese, Thomas Funkhouser
								<br>
								<i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2018</i><br>
                                <font color="4e79a7"><i>Oral Presentation</i></font><br>
                                    <a href="http://im2pano3d.cs.princeton.edu">Project</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
									<a href="https://arxiv.org/pdf/1712.04569.pdf">PDF</a>
								</p>
							</article>
							<article class="12u 12u$(xsmall) work-item">
								<a href="https://niessner.github.io/Matterport" class="image fit thumb left"><img src="img/proj/rs/matterport/1.jpg" alt="" /></a>
								<h3 style="margin-left: 43%">Matterport3D: Learning from RGB-D Data in Indoor Environments</h3>
								<p style="margin-left: 43%">
								We introduce Matterport3D, a large-scale RGB-D dataset containing 10,800 panoramic views from 194,400 RGB-D images of 90 building-scale scenes. Annotations are provided with surface reconstructions, camera poses, and 2D and 3D semantic segmentations. The precise global alignment and comprehensive, diverse panoramic set of views over entire buildings enable a variety of supervised and self-supervised computer vision tasks, including keypoint matching, view overlap prediction, normal prediction from color, semantic segmentation, and scene classification.
								</p>
								<p style="margin-left: 43%; margin-top: 1em">
								Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Nießner, Manolis Savva, Shuran Song, <font color="49bf9d">Andy Zeng</font>, Yinda Zhang
								<br>
								<i>IEEE International Conference on 3D Vision (3DV) 2017</i><br>
                                    <a href="https://niessner.github.io/Matterport">Project</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
									<a href="https://arxiv.org/pdf/1709.06158.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
									<a href="https://github.com/niessner/Matterport">Code (Github)</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
									<a href="https://matterport.com/blog/2017/09/20/announcing-matterport3d-research-dataset/">Matterport Blog</a>
								</p>
							</article>
							<article class="12u 12u$(xsmall) work-item">
								<a href="http://3dmatch.cs.princeton.edu/" class="image fit thumb left"><img src="img/proj/rs/3dmatch/teaser.jpg" alt="" /></a>
								<h3 style="margin-left: 43%">3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions</h3>
								<p style="margin-left: 43%">
								We present a data-driven model that learns a local 3D shape descriptor for establishing correspondences between partial and noisy 3D/RGB-D data. To amass training data for our model, we propose an unsupervised feature learning method that leverages the millions of correspondence labels found in existing RGB-D reconstructions. Our learned descriptor is not only able to match local geometry in new scenes for reconstruction, but also generalize to different tasks and spatial scales (e.g. instance-level object model alignment for the Amazon Picking Challenge, and mesh surface correspondence). 
								</p>
								<p style="margin-left: 43%; margin-top: 1em">
								<font color="49bf9d">Andy Zeng</font>, Shuran Song, Matthias Nießner, Matthew Fisher, Jianxiong Xiao, Thomas Funkhouser
								<br>
								<i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017</i><br>
								<font color="4e79a7"><i>Oral Presentation</i></font><br>
	                                <a href="http://3dmatch.cs.princeton.edu/">Project</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
									<a href="https://arxiv.org/pdf/1603.08182.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
									<a href="https://github.com/andyzeng/3dmatch-toolbox">Code (Github)</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
									<a href="https://www.youtube.com/watch?v=qNVZl7bCjsU&list=PL_bDvITUYucADb15njRd7geem8vxOyo6N&index=3">Talk</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
									<a href="https://www.youtube.com/watch?v=1U3YKnuMS7g">2 Minute Papers</a>
								</p>
							</article>
							<article class="12u 12u$(xsmall) work-item">
								<a href="http://sscnet.cs.princeton.edu/" class="image fit thumb left"><img src="http://vision.princeton.edu/projects/2016/SSCNet/thumbnail.jpg" alt="" /></a>
								<h3 style="margin-left: 43%">Semantic Scene Completion from a Single Depth Image</h3>
								<p style="margin-left: 43%">
								We present an end-to-end model that is capable of inferring a complete 3D voxel representation of volumetric occupancy and semantic labels for a scene from a single-view depth map observation. To train our model, we construct SUNCG -- a manually created large-scale dataset of synthetic 3D scenes with dense volumetric annotations.
								</p>
								<p style="margin-left: 43%; margin-top: 1em">
								Shuran Song, Fisher Yu, <font color="49bf9d">Andy Zeng</font>, Angel X. Chang, Manolis Savva, Thomas Funkhouser
								<br>
								<i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2017</i><br>
								<font color="4e79a7"><i>Oral Presentation</i></font><br>
	                                <a href="http://sscnet.cs.princeton.edu/">Project</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
									<a href="https://arxiv.org/pdf/1611.08974.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                                    <a href="http://suncg.cs.princeton.edu/">SUNCG Dataset</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                                    <a href="https://github.com/shurans/sscnet">Code (Github)</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
									<a href="https://www.youtube.com/watch?v=Aq7hLLIz5a0&list=PL_bDvITUYucADb15njRd7geem8vxOyo6N&index=2">Talk</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
									<a href="https://www.youtube.com/watch?v=8YWgar0uCF8">2 Minute Papers</a>
								</p>
							</article>
							<article class="12u 12u$(xsmall) work-item">
								<a href="http://apc.cs.princeton.edu/" class="image fit thumb left"><img src="img/proj/rs/apc/teaser_v4.jpg" alt="" /></a>
								<h3 style="margin-left: 43%">Multi-view Self-supervised Deep Learning for 6D Pose Estimation in the Amazon Picking Challenge</h3>
								<p style="margin-left: 43%">

								We developed a vision system that can recognize objects and estimate their 6D poses under cluttered environments, partial data, sensor noise, multiple instances of the same object, and a large variety of object categories. Our approach leverages fully convolutional networks to segment and label multiple RGB-D views of a scene, then fits pre-scanned 3D object models to the resulting segmentation to estimate their poses. We also propose a scalable self-supervised method that leverages precise and repeatable robot motions to generate a large labeled dataset without tedious manual annotations. The approach was part of the MIT-Princeton Team system that took <font color="4e79a7">3<sup>rd</sup> place at the 2016 Amazon Picking Challenge.</font> 
								</p>
								<p style="margin-left: 43%; margin-top: 1em">
								<font color="49bf9d">Andy Zeng</font>, Kuan-Ting Yu, Shuran Song, Daniel Suo, Ed Walker Jr., Alberto Rodriguez, Jianxiong Xiao
								<br>
								<i>IEEE International Conference on Robotics and Automation (ICRA) 2017</i><br>
	                                <a href="http://apc.cs.princeton.edu/">Project</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
									<a href="https://arxiv.org/pdf/1609.09475.pdf">PDF</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                                    <a href="http://apc.cs.princeton.edu/#shelf-and-tote-benchmark-dataset">Shelf &amp; Tote Dataset</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;
                                    <a href="https://github.com/andyzeng/apc-vision-toolbox">Code (Github)</a>
								</p>
							</article>
						</div>
						<hr/>
						<h2>Honors</h2>
						My research has been graciously funded by
						<ul>
                            <li><a href="https://blogs.nvidia.com/blog/2018/04/04/nvidia-graduate-fellowship-program/">NVIDIA Fellowship</a> (2018 - 2019)</li>
                            <li><a href="https://ara.amazon-ml.com/">Amazon Research Award</a> (2018)</li>
							<li><a href="http://giving.princeton.edu/scholarships-fellowships/fellowships/endowed2">Gordon Y.S. Wu Fellowship in Engineering and Wu Prize</a> (2015 - 2016)</li>
	                	</ul>
						<hr/>
						<p style="margin-bottom:0">
						<br>
						<br>
						<br>
						<sub>Design: <a href="http://html5up.net">HTML5 UP</a>&nbsp;&nbsp;&bull;&nbsp;&nbsp;My old website resides <a href="old.html">here</a></sub>
						</p>
					</section>
			</div>

		<!-- Footer -->
			<footer id="footer">
				<div class="inner">
					<ul class="copyright">
						<li>Meet <a href="https://en.wikipedia.org/wiki/Danbo_(character)">Danbo</a> the cardboard robot.</li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.poptrox.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>

	</body>
</html>
